{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# model\n",
    "llm = Ollama(model=\"gemma:latest\")\n",
    "\n",
    "# chain 실행\n",
    "response = llm.invoke(\"지구의 자전 주기는?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지구의 자전 주기는 **24시간**입니다.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"You are an expert in astronomy. Answer the question. <Question>: {input}\")\n",
    "\n",
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm\n",
    "\n",
    "# chain 호출\n",
    "response = chain.invoke({\"input\": \"지구의 자전 주기는?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지구의 자전 주기는 **24 시간**입니다. 지구는 자전축을 중심으로 회전하며, 하나의 완전회전을 하기 위해 약 24 시간이 걸립니다.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groq AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ChatGroq 모델 초기화\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 정의\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 친절하고 유익한 AI 전문가입니다.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain 생성\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: groq이 뭔가요?\n",
      "답변: Groq은 구글에서 개발한 고성능 컴퓨팅 칩입니다. 2020년에 공개되었으며, 기존의 중앙 처리 장치(CPU)와 그래픽 처리 장치(GPU)보다 더 빠르고 효율적인 컴퓨팅을 제공하는 것을 목표로 합니다.\n",
      "\n",
      "Groq 칩은 구글의 텐서 프로세싱 유닛(TPU) 기술을 기반으로 하며, 기계 학습과 딥 러닝 작업을 위해 특별히 설계되었습니다. Groq 칩은 매우 큰 규모의 컴퓨팅 작업을 수행할 수 있으며, 특히 이미지와 비디오 처리, 자연어 처리, 추천 시스템 등과 같은 작업에서 뛰어난 성능을 발휘합니다.\n",
      "\n",
      "Groq 칩의 주요 특징은 다음과 같습니다:\n",
      "\n",
      "1. **고성능**: Groq 칩은 매우 빠른 컴퓨팅 성능을 제공하며, 기존의 CPU와 GPU보다 더 빠른 성능을 제공합니다.\n",
      "2. **저전력**: Groq 칩은 저전력 설계로, 에너지 소모를 최소화하여 데이터 센터와 엣지 컴퓨팅 환경에서 사용하기 적합합니다.\n",
      "3. **스케일링**: Groq 칩은 대규모 컴퓨팅 작업을 수행할 수 있으며, 여러 칩을 연결하여 더 큰 규모의 컴퓨팅을 수행할\n",
      "\n",
      "질문: 가장 성능이 좋은 오픈 소스 LLM 모델은 뭔가요?\n",
      "답변: 현재 가장 성능이 좋은 오픈 소스 LLM(Large Language Model) 모델은 여러 가지 요인에 따라 달라질 수 있지만, 최근에는 다음과 같은 모델들이 주목을 받고 있습니다.\n",
      "\n",
      "1. **LLaMA**: Meta AI에서 개발한 LLaMA 모델은 최근에 공개된 오픈 소스 모델로, 다양한 크기(7B, 13B, 33B, 65B)로 제공됩니다. LLaMA는 자연어 이해, 생성, 번역 등 다양한 태스크에서 높은 성능을 보여주고 있습니다.\n",
      "2. **BLOOM**: BLOOM은 빅테크(BigTech)와 연구 기관이 공동으로 개발한 모델로, 176B의 크기로 알려져 있습니다. 이 모델은 다양한 언어를 지원하며, 자연어 이해, 생성, 번역 등 다양한 태스크에서 높은 성능을 보여주고 있습니다.\n",
      "3. **OPT**: OPT(Optimal Transformer) 모델은 Meta AI에서 개발한 모델로, 175B의 크기로 알려져 있습니다. 이 모델은 자연어 이해, 생성, 번역 등 다양한 태스크에서 높은 성능을 보여주고 있습니다.\n",
      "4. **ERNIE**: ERNIE(Enhanced Representation through kNowledge Integration) 모델은 百度(Baidu)에서 개발한 모델로, 다양한 크기(100M, 1B, 3B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 질문 리스트\n",
    "# 질문 리스트\n",
    "questions = [\n",
    "    \"groq이 뭔가요?\",\n",
    "    \"가장 성능이 좋은 오픈 소스 LLM 모델은 뭔가요?\"\n",
    "]\n",
    "\n",
    "# 각 질문에 대한 답변 생성\n",
    "for question in questions:\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    print(f\"질문: {question}\")\n",
    "    print(f\"답변: {response.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
